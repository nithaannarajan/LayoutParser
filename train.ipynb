{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The script is based on https://github.com/facebookresearch/detectron2/blob/master/tools/train_net.py. \n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import detectron2.utils.comm as comm\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
    "\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "from detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, hooks, launch\n",
    "from detectron2.evaluation import (\n",
    "    COCOEvaluator,\n",
    "    verify_results,\n",
    ")\n",
    "from detectron2.modeling import GeneralizedRCNNWithTTA\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_augs(cfg):\n",
    "    \"\"\"Add all the desired augmentations here. A list of availble augmentations\n",
    "    can be found here: \n",
    "       https://detectron2.readthedocs.io/en/latest/modules/data_transforms.html\n",
    "    \"\"\"\n",
    "    augs = [\n",
    "        T.ResizeShortestEdge(\n",
    "            cfg.INPUT.MIN_SIZE_TRAIN, cfg.INPUT.MAX_SIZE_TRAIN, cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING\n",
    "        )\n",
    "    ]\n",
    "    if cfg.INPUT.CROP.ENABLED:\n",
    "        augs.append(\n",
    "            T.RandomCrop_CategoryAreaConstraint(\n",
    "                cfg.INPUT.CROP.TYPE,\n",
    "                cfg.INPUT.CROP.SIZE,\n",
    "                cfg.INPUT.CROP.SINGLE_CATEGORY_MAX_AREA,\n",
    "                cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,\n",
    "            )\n",
    "        )\n",
    "    horizontal_flip: bool = (cfg.INPUT.RANDOM_FLIP == 'horizontal')\n",
    "    augs.append(T.RandomFlip(horizontal=horizontal_flip,\n",
    "                             vertical=not horizontal_flip))\n",
    "    # Rotate the image between -90 to 0 degrees clockwise around the centre\n",
    "    augs.append(T.RandomRotation(angle=[-90.0, 0.0]))\n",
    "    return augs\n",
    "\n",
    "class Trainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    We use the \"DefaultTrainer\" which contains pre-defined default logic for\n",
    "    standard training workflow. They may not work for you, especially if you\n",
    "    are working on a new research project. In that case you can use the cleaner\n",
    "    \"SimpleTrainer\", or write your own training loop. You can use\n",
    "    \"tools/plain_train_net.py\" as an example.\n",
    "    \n",
    "    Adapted from:\n",
    "        https://github.com/facebookresearch/detectron2/blob/master/projects/DeepLab/train_net.py\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        mapper = DatasetMapper(cfg, is_train=True, augmentations=get_augs(cfg))\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            DatasetEvaluator or None\n",
    "        It is not implemented by default.\n",
    "        \"\"\"\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "\n",
    "    @classmethod\n",
    "    def test_with_TTA(cls, cfg, model):\n",
    "        logger = logging.getLogger(\"detectron2.trainer\")\n",
    "        # In the end of training, run an evaluation with TTA\n",
    "        # Only support some R-CNN models.\n",
    "        logger.info(\"Running inference with test-time augmentation ...\")\n",
    "        model = GeneralizedRCNNWithTTA(cfg, model)\n",
    "        evaluators = [\n",
    "            cls.build_evaluator(\n",
    "                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n",
    "            )\n",
    "            for name in cfg.DATASETS.TEST\n",
    "        ]\n",
    "        res = cls.test(cfg, model, evaluators)\n",
    "        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def eval_and_save(cls, cfg, model):\n",
    "        evaluators = [\n",
    "            cls.build_evaluator(\n",
    "                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "            )\n",
    "            for name in cfg.DATASETS.TEST\n",
    "        ]\n",
    "        res = cls.test(cfg, model, evaluators)\n",
    "        pd.DataFrame(res).to_csv(os.path.join(cfg.OUTPUT_DIR, 'eval.csv'))\n",
    "        return res\n",
    "\n",
    "def setup(args):\n",
    "    \"\"\"\n",
    "    Create configs and perform basic setups.\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    if args.config_file != \"\":\n",
    "        cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "\n",
    "    with open(args.json_annotation_train, 'r') as fp:\n",
    "        anno_file = json.load(fp)\n",
    "\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(anno_file[\"categories\"])\n",
    "    del anno_file\n",
    "\n",
    "    cfg.DATASETS.TRAIN = (f\"{args.dataset_name}-train\",)\n",
    "    cfg.DATASETS.TEST = (f\"{args.dataset_name}-val\",)\n",
    "\n",
    "    cfg.freeze()\n",
    "    default_setup(cfg, args)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    cfg = setup(args)\n",
    "\n",
    "    if args.eval_only:\n",
    "        model = Trainer.build_model(cfg)\n",
    "        DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
    "            cfg.MODEL.WEIGHTS, resume=args.resume\n",
    "        )\n",
    "        res = Trainer.test(cfg, model)\n",
    "    \n",
    "        if cfg.TEST.AUG.ENABLED:\n",
    "            res.update(Trainer.test_with_TTA(cfg, model))\n",
    "        if comm.is_main_process():\n",
    "            verify_results(cfg, res)\n",
    "\n",
    "        # Save the evaluation results\n",
    "        pd.DataFrame(res).to_csv(f'{cfg.OUTPUT_DIR}/eval.csv')\n",
    "        return res\n",
    "\n",
    "    # Ensure that the Output directory exists\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    \"\"\"\n",
    "    If you'd like to do anything fancier than the standard training logic,\n",
    "    consider writing your own training loop (see plain_train_net.py) or\n",
    "    subclassing the trainer.\n",
    "    \"\"\"\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.resume_or_load(resume=args.resume)\n",
    "    trainer.register_hooks(\n",
    "            [hooks.EvalHook(0, lambda: trainer.eval_and_save(cfg, trainer.model))]\n",
    "    )\n",
    "    if cfg.TEST.AUG.ENABLED:\n",
    "        trainer.register_hooks(\n",
    "            [hooks.EvalHook(0, lambda: trainer.test_with_TTA(cfg, trainer.model))]\n",
    "        )\n",
    "    return trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = default_argument_parser()\n",
    "\n",
    "    # Extra Configurations for dataset names and paths\n",
    "    parser.add_argument(\"--dataset_name\",          default=\"\", help=\"The Dataset Name\")\n",
    "    parser.add_argument(\"--json_annotation_train\", default=\"\", metavar=\"FILE\", help=\"The path to the training set JSON annotation\")\n",
    "    parser.add_argument(\"--image_path_train\",      default=\"\", metavar=\"FILE\", help=\"The path to the training set image folder\")\n",
    "    parser.add_argument(\"--json_annotation_val\",   default=\"\", metavar=\"FILE\", help=\"The path to the validation set JSON annotation\")\n",
    "    parser.add_argument(\"--image_path_val\",        default=\"\", metavar=\"FILE\", help=\"The path to the validation set image folder\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"Command Line Args:\", args)\n",
    "\n",
    "    # Register Datasets \n",
    "    dataset_name = args.dataset_name\n",
    "    register_coco_instances(f\"{dataset_name}-train\", {}, \n",
    "                            args.json_annotation_train, \n",
    "                            args.image_path_train)\n",
    "\n",
    "    register_coco_instances(f\"{dataset_name}-val\", {}, \n",
    "                            args.json_annotation_val,   \n",
    "                            args.image_path_val)\n",
    "\n",
    "    launch(\n",
    "        main,\n",
    "        args.num_gpus,\n",
    "        num_machines=args.num_machines,\n",
    "        machine_rank=args.machine_rank,\n",
    "        dist_url=args.dist_url,\n",
    "        args=(args,),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
